{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:-  ['this', 'is', 'the', 'first', 'test', 'for', 'reading', 'the', 'text', 'and', 'i', 'feel', 'good', 'now']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import string\n",
    "text = open('readfile.txt',encoding='utf-8').read();\n",
    "lower_case = text.lower() #converting the text into lowercase \n",
    "#lowercase is just a function conver anything to strings removing the punctuations \n",
    "#Cleaning the text\n",
    "cleaned_text = lower_case.translate(str.maketrans('','',string.punctuation))\n",
    "#maketrans eg str1 = abc str2 =gef we need to remove them so we write maketrans(str1,str2, delete this things ) in our case we just need to delete the puncutations \n",
    "#and translate is just converting\n",
    "tokenized_words = cleaned_text.split()\n",
    "print(\"Output:- \",tokenized_words)\n",
    "#split make sure that the words which we get are sepreated in a list\n",
    "#NLP is analysis of words not the sentences so that we can use it later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'test', 'reading', 'text', 'feel', 'good']\n"
     ]
    }
   ],
   "source": [
    "#Removing the stop words(Anything which do not add any meaning to the sentences)\n",
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
    "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "# now that we have a list of words which have no meaning to the sentence we just can remove it from our sentence\n",
    "final_words = [] #it is just a list of words that we end up after removing the stop words\n",
    "for word in tokenized_words:\n",
    "    if word not in stop_words:\n",
    "        final_words.append(word)\n",
    "#what we are doing is looping the tokenized words and then storing it into a temp. variable named as word\n",
    "#if the current word is not prensent in the stop_words list then we can just append it to the final_words list or simply saying adding the words to the final words\n",
    "print(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP ALGORITHM\n",
    "#1)check the word if it is prensent in emotion.txt\n",
    "#2)if word is present->add it to emotion_list\n",
    "#3)Finally count each emotion in the emotion list\n",
    "\n",
    "with open('emotions.txt','r') as file:\n",
    "    for line in file:\n",
    "       clear_line = line.replace('\\n','').replace(\",\",'').replace(\"'\", '').strip() #clearing line comma and spaces\n",
    "     # print(clear_line)\n",
    "       word,emotion =clear_line.split(':')# it is just spliting words and emotion for eg accused : cheated so accused will be in word and cheated will be the emotion  \n",
    "      ## print(emotion)\n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-75-ee7a31495523>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-75-ee7a31495523>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    if word in final_words:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "emotion_list=[]\n",
    "   \n",
    "    if word in final_words:\n",
    "    emotion_list.append(emotion)\n",
    "print(emotion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
