{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:-  ['game', 'of', 'thrones', 'is', 'an', 'american', 'fantasy', 'drama', 'television', 'series', 'created', 'by', 'david', 'benioff', 'and', 'd', 'b', 'weiss', 'for', 'hbo', 'it', 'is', 'an', 'adaptation', 'of', 'a', 'song', 'of', 'ice', 'and', 'fire', 'a', 'series', 'of', 'fantasy', 'novels', 'by', 'george', 'r', 'r', 'martin', 'the', 'first', 'of', 'which', 'is', 'a', 'game', 'of', 'thrones', 'the', 'show', 'was', 'shot', 'in', 'the', 'united', 'kingdom', 'canada', 'croatia', 'iceland', 'malta', 'morocco', 'and', 'spain', 'it', 'premiered', 'on', 'hbo', 'in', 'the', 'united', 'states', 'on', 'april', '17', '2011', 'and', 'concluded', 'on', 'may', '19', '2019', 'with', '73', 'episodes', 'broadcast', 'over', 'eight', 'seasons', 'set', 'on', 'the', 'fictional', 'continents', 'of', 'westeros', 'and', 'essos', 'game', 'of', 'thrones', 'has', 'a', 'large', 'ensemble', 'cast', 'and', 'follows', 'several', 'story', 'arcs', 'throughout', 'the', 'course', 'of', 'the', 'show', 'a', 'major', 'arc', 'concerns', 'the', 'iron', 'throne', 'of', 'the', 'seven', 'kingdoms', 'of', 'westeros', 'and', 'follows', 'a', 'web', 'of', 'alliances', 'and', 'conflicts', 'among', 'the', 'noble', 'dynasties', 'either', 'vying', 'to', 'claim', 'the', 'throne', 'or', 'fighting', 'for', 'independence', 'from', 'it', 'another', 'focuses', 'on', 'the', 'last', 'descendant', 'of', 'the', 'realms', 'deposed', 'ruling', 'dynasty', 'who', 'has', 'been', 'exiled', 'to', 'essos', 'and', 'is', 'plotting', 'a', 'return', 'to', 'the', 'throne', 'a', 'third', 'story', 'arc', 'follows', 'the', 'nights', 'watch', 'a', 'military', 'order', 'defending', 'the', 'realm', 'against', 'threats', 'from', 'the', 'north', 'game', 'of', 'thrones', 'attracted', 'a', 'record', 'viewership', 'on', 'hbo', 'and', 'has', 'a', 'broad', 'active', 'and', 'international', 'fan', 'base', 'critics', 'have', 'praised', 'the', 'series', 'for', 'its', 'acting', 'complex', 'characters', 'story', 'scope', 'and', 'production', 'values', 'although', 'its', 'frequent', 'use', 'of', 'nudity', 'and', 'violence', 'including', 'sexual', 'violence', 'has', 'been', 'subject', 'to', 'criticism', 'the', 'final', 'season', 'received', 'significant', 'critical', 'backlash', 'for', 'its', 'reduced', 'length', 'and', 'creative', 'decisions', 'with', 'many', 'considering', 'it', 'a', 'disappointing', 'conclusion', 'the', 'series', 'received', '59', 'primetime', 'emmy', 'awards', 'the', 'most', 'by', 'a', 'drama', 'series', 'including', 'outstanding', 'drama', 'series', 'in', '2015', '2016', '2018', 'and', '2019', 'its', 'other', 'awards', 'and', 'nominations', 'include', 'three', 'hugo', 'awards', 'for', 'best', 'dramatic', 'presentation', 'a', 'peabody', 'award', 'and', 'five', 'nominations', 'for', 'the', 'golden', 'globe', 'award', 'for', 'best', 'television', 'series', 'â€“', 'drama', 'many', 'critics', 'and', 'publications', 'have', 'named', 'the', 'show', 'as', 'one', 'of', 'the', 'best', 'television', 'series', 'of', 'all', 'time']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "\n",
    "text = open('readfile.txt',encoding='utf-8').read();\n",
    "lower_case = text.lower() #converting the text into lowercase \n",
    "#lowercase is just a function conver anything to strings removing the punctuations \n",
    "#Cleaning the text\n",
    "cleaned_text = lower_case.translate(str.maketrans('','',string.punctuation))\n",
    "#maketrans eg str1 = abc str2 =gef we need to remove them so we write maketrans(str1,str2, delete this things ) in our case we just need to delete the puncutations \n",
    "#and translate is just converting\n",
    "tokenized_words = cleaned_text.split()\n",
    "print(\"Output:- \",tokenized_words)\n",
    "#split make sure that the words which we get are sepreated in a list\n",
    "#NLP is analysis of words not the sentences so that we can use it later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the stop words(Anything which do not add any meaning to the sentences)\n",
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
    "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "# now that we have a list of words which have no meaning to the sentence we just can remove it from our sentence\n",
    "final_words = [] #it is just a list of words that we end up after removing the stop words\n",
    "for word in tokenized_words:\n",
    "    if word not in stop_words:\n",
    "        final_words.append(word)\n",
    "#what we are doing is looping the tokenized words and then storing it into a temp. variable named as word\n",
    "#if the current word is not prensent in the stop_words list then we can just append it to the final_words list or simply saying adding the words to the final words\n",
    "#print(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' attracted', ' esteemed']\n",
      "Counter({' attracted': 1, ' esteemed': 1})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NLP ALGORITHM\n",
    "#1)check the word if it is prensent in emotion.txt\n",
    "#2)if word is present->add it to emotion_list\n",
    "#3)Finally count each emotion in the emotion list\n",
    "emotion_list = []\n",
    "with open('emotions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        clear_line = line.replace(\"\\n\", '').replace(\",\", '').replace(\"'\", '').strip()\n",
    "        word, emotion = clear_line.split(':')# it is just spliting words and emotion for eg accused : cheated so accused will be in word and cheated will be the emotion  \n",
    "\n",
    "        if word in final_words:\n",
    "            emotion_list.append(emotion)\n",
    "            \n",
    "print(emotion_list)\n",
    "count = Counter(emotion_list)\n",
    "print(count)\n",
    "plt.bar(count.keys(),count.values())\n",
    "plt.savefig('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
